{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Monad for collect logs in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We're going to import the libraries that we're going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` \n",
    "import $ivy.`org.apache.spark::spark-core:2.4.3` \n",
    "import $ivy.`org.typelevel::cats-core:2.3.0` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to compare the shema from a dataframe and from a List of columns provide. We want to check that all the columns in the list are in the dataframe. And for example we're going to check that at least one of the columns is mandatory (is only for show how you can use more than one Writer together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two case classes that we need for our validation method that we're going to use for validate the schema from a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSchemaColumn\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mValidationLog\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class SchemaColumn(\n",
    "                         name: String,\n",
    "                         mandatory: Boolean = true\n",
    "                       )\n",
    "case class ValidationLog (\n",
    "                              log: String,\n",
    "                              validation: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to see how can we do it without FP, with side effects and mutable variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvalidation\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validation(df: DataFrame, schema: List[SchemaColumn]): Boolean = {\n",
    "    var message = \"\"\n",
    "    val checkColumnsNames = df.schema.names.map(_.toUpperCase).forall { name =>\n",
    "      val check = schema.map(_.name.toUpperCase).contains(name)\n",
    "      if(!check) message = message + s\" Column: $name not in the registration. \"\n",
    "      check\n",
    "    }\n",
    "    val checkMandatory = schema.exists(_.mandatory.equals(true))\n",
    "      if(!checkMandatory) {message = message + \" No mandatory Columns. \"}\n",
    "    \n",
    "      println(message)\n",
    "      if(checkColumnsNames && checkMandatory) true else false\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go, we're going to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/05/19 15:21:29 INFO Server: jetty-9.4.30.v20200611; built: 2020-06-11T12:34:51.929Z; git: 271836e4c1f4612f12b7bb13ef5a92a927634b0d; jvm 1.8.0_265-8u265-b01-0ubuntu2~20.04-b01\n",
      "21/05/19 15:21:29 INFO AbstractConnector: Started ServerConnector@54efe60d{HTTP/1.1, (http/1.1)}{172.17.0.2:40757}\n",
      "21/05/19 15:21:29 INFO Server: Started @2805179ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/05/19 15:21:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/notebooks/spark-warehouse').\n",
      "21/05/19 15:21:29 INFO SharedState: Warehouse path is 'file:/home/jovyan/notebooks/spark-warehouse'.\n",
      "21/05/19 15:21:29 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "21/05/19 15:21:29 WARN NotebookSparkSessionBuilder: Using an existing SparkSession; some configuration may not take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://6c0c648b7d31:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@60ba48d4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36msomeDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [number: int, word: string]\n",
       "\u001b[36mschemaColumns\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mSchemaColumn\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mSchemaColumn\u001b[39m(\u001b[32m\"column_that_fail\"\u001b[39m, false),\n",
       "  \u001b[33mSchemaColumn\u001b[39m(\u001b[32m\"word\"\u001b[39m, false)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val someDF = Seq(\n",
    "  (8, \"bat\"),\n",
    "  (64, \"mouse\"),\n",
    "  (-27, \"horse\")\n",
    ").toDF(\"number\", \"word\")\n",
    "\n",
    "val schemaColumns = List(SchemaColumn(\"column_that_fail\",false), SchemaColumn(\"word\", false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column: NUMBER not in the registration.  No mandatory Columns. \n",
      "false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalid\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val valid = validation(someDF, schemaColumns)\n",
    "println(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the validation failed and we print the number column because is not in the Schema and that we don't have mandatory columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that is more pretty is you use a BufferList or something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcats.data._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcats.implicits.catsKernelStdMonoidForString\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcats.instances._\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cats.data._\n",
    "import cats.implicits.catsKernelStdMonoidForString\n",
    "import cats.instances._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to do something similar but with the Monad Writer from the cats library. Without side effect or mutable variables (good for parallel and concurrent computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvalidationF\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def validationF(df: DataFrame, schema: List[SchemaColumn]): ValidationLog = {\n",
    "    val checkColumns = df.schema.names.map(_.toUpperCase).foldLeft(Writer(\"\",true)){ (wr, name) =>\n",
    "      schema.map(_.name.toUpperCase).contains(name) match {\n",
    "        case true => wr\n",
    "        case false => wr.tell(s\" Column: $name not in the registration. \").map(_ => false)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    val checkMandatory = (schema.exists(_.mandatory.equals(true)) match {\n",
    "      case true => Writer(\"\",true)\n",
    "      case false => Writer(\"No mandatory Columns in the registration\",false)\n",
    "    })\n",
    "\n",
    "    val check = (for {\n",
    "      columns <- checkColumns\n",
    "      mandatory  <- checkMandatory\n",
    "    } yield if(columns && mandatory) true else false).run\n",
    "\n",
    "    ValidationLog tupled check\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalid\u001b[39m: \u001b[32mValidationLog\u001b[39m = \u001b[33mValidationLog\u001b[39m(\n",
       "  \u001b[32m\" Column: NUMBER not in the registration. No mandatory Columns in the registration\"\u001b[39m,\n",
       "  false\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val valid = validationF(someDF, schemaColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have encapsulate the log from the method in the Left side of the Writer monad and in the right side you will have the booelan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column: NUMBER not in the registration. No mandatory Columns in the registration\n"
     ]
    }
   ],
   "source": [
    "println(valid.log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
